# Attribute Aggregation and Transformation
import yaml
import pandas as pd
from pathlib import Path
import fs_algo.fs_algo_train_eval as fsate
from collections.abc import Iterable

from typing import Callable
import itertools
import numpy as np
import dask.dataframe as dd
from datetime import datetime, timezone
import os
from collections import ChainMap


def read_df_ext(path_to_file: str | os.PathLike) -> pd.DataFrame:
    """Read a tabular file with an extension of csv or parquet

    :param path_to_file: file path of tabular file
    :type path_to_file: str | os.PathLike
    :raises ValueError: f-string formatting still pressent in `path_to_file`
    :raises ValueError: File could not be read as expected format
    :return: tabular dataframe of file contents
    :rtype: pd.DataFrame
    """
    path_to_file = Path(path_to_file)
    if '{' in str(path_to_file):
        raise ValueError("The following path still contains f-string formatting" +
                          f" & needs rectified:\n {path_to_file}")
    if 'csv' in path_to_file.suffix:
        df = pd.read_csv(path_to_file)
    elif 'parquet' in path_to_file.suffix:
        df = pd.read_parquet(path_to_file)
    else:
        raise ValueError("Expecting path to file containing comids to be csv or parquet file")
    return df


def _get_comids_std_attrs(path_attr_config: str | os.PathLike, 
                          likely_ds_types: list =['training','prediction'], 
                          loc_id_col: str = 'comid') -> list:
    """Retrieve comids from the standardized attribute metadata generated
      by proc.attr.hydfab R package processing

    :param path_attr_config: File path to the attribute config file
    :type path_attr_config: str | os.PathLike
    :param likely_ds_types: Very likely dataset types used in the f-string
      formated metadata filename, `path_metadata`, defaults to ['training','prediction']
    :type likely_ds_types: list, optional
    :param loc_id_col: The location ID column name in the metadata tabular file,
      defaults to 'comid'
    :type loc_id_col: str, optional
    :raises Warning: In case no comid data found. This function shouldn't be called if no data desired.
    :return: list of comids corresponding to standardized attributes
    :rtype: list
    """
    # Initialize attribute configuration class for extracting attributes
    attr_cfig = fsate.AttrConfigAndVars(path_attr_config)
    attr_cfig._read_attr_config()

    fio_attr = dict(ChainMap(*attr_cfig.attr_config.get('file_io')))

    # items in attrs_cfg_dict have already been evaluated for f-strings
    datasets = attr_cfig.attrs_cfg_dict.get('datasets') # Identify datasets of interest
    dir_base = attr_cfig.attrs_cfg_dict.get('dir_base') # Possibly used for f-string eval with path_meta
    dir_std_base = attr_cfig.attrs_cfg_dict.get('dir_std_base') # Possibly used for f-string eval with path_meta

    write_type = fio_attr.get('write_type') # Likely used for f-string eval with path_meta
    ds_type_attr = fio_attr.get('ds_type') # Likely used for f-string eval with path_meta
    # These are the likely ds type names. Check to see if files with these names also exist once defining path_meta below.
    likely_ds_types=list(set(likely_ds_types+[ds_type_attr]))

    ls_comids_attrs = list()
    for ds in datasets: # ds likely used for f-string eval with path_meta
        for ds_type in likely_ds_types: # ds_type likely used for f-string eval with path_meta
            path_meta = Path(eval(f"f'{fio_attr.get('path_meta')}'"))
            if path_meta.exists:
                print(f"Reading {path_meta}")
                df_meta = read_df_ext(path_meta)
                ls_comids_attrs = ls_comids_attrs + df_meta[loc_id_col].to_list()
    if len(ls_comids_attrs) == 0:
        raise Warning(f"Unexpectedly, no data found reading standardized metadata generated by basin attribute grabbing workflow.")
        
    return ls_comids_attrs

#%%                      CUSTOM ATTRIBUTE AGGREGATION
# Function to convert a string representing a function name into a function object
def _get_function_from_string(func_str: str) -> Callable:
    if '.' in func_str:
        module_name, func_name = func_str.rsplit('.', 1)  # Split into module and function
        module = globals().get(module_name)               # Get module object from globals()
        if module:
            func = getattr(module, func_name)             # Get function object from module
    else:
        func = eval(func_str)
    return func

def _std_attr_filepath(dir_db_attrs: str | os.PathLike,
                       comid: str,
                       attrtype:str=['attr','tfrmattr','cstmattr'][0]
                       ) -> Path:
    """Make a standardized attribute filepath

    :param dir_db_attrs: Directory path containing attribute .parquet files
    :type dir_db_attrs: str | os.PathLike
    :param comid: USGS NHDplus common identifier for a catchment
    :type comid: str
    :param attrtype: the type of attribute, defaults to 'attr'
    Options include 'attr' for a publicly-available, easily retrievable 
    attribute acquired via the R package proc.attr.hydfab
    'tfrmattr' for a transformed attribute, and 
    'cstmattr' for an attribute from a custom dataset
    :type attrtype: str, optional
    :return: Full filepath of the new attribute for a single comid
    :rtype: Path
    """
    
    new_file_name = Path(f'comid_{comid}_{attrtype}.parquet')
    new_path = Path(Path(dir_db_attrs)/new_file_name)
    return new_path

def io_std_attrs(df_new_vars: pd.DataFrame,
                    dir_db_attrs:str | os.PathLike,
                    comid:str, 
                    attrtype:str)->pd.DataFrame:
    """Write/update attributes corresponding to a single comid location

    :param df_new_vars: The new variables corresponding to a catchment
    :type df_new_vars: pd.DataFrame
    :param dir_db_attrs: Directory of attribute data
    :type dir_db_attrs: str | os.PathLike
    :param comid: USGS NHDplus common identifier for a catchment
    :type comid: str
    :param attrtype: The type of attribute data. Expected to be 'attr', 'tfrmattr', or 'cstmattr'
    :type attrtype: str
    :return: The full attribute dataframe for a given catchment
    :rtype: pd.DataFrame
    """
    if df_new_vars.shape[0] > 0:
        
        # Create the expected transformation data filepath path
        path_tfrm_comid = _std_attr_filepath(dir_db_attrs=dir_db_attrs,
                        comid=comid,
                        attrtype = 'tfrmattr')
        
        if path_tfrm_comid.exists():
            print(f"Updating {path_tfrm_comid}")
            df_exst_vars_tfrm = pd.read_parquet(path_tfrm_comid)
            # Append new variables
            df_new_vars = pd.concat([df_exst_vars_tfrm,df_new_vars])
        else:
            print(f"Writing {path_tfrm_comid}")
        
        df_new_vars.to_parquet(path_tfrm_comid,index=False)
        
    return df_new_vars

def _subset_ddf_parquet_by_comid(dir_db_attrs: str | os.PathLike,
                                  fp_struct:str 
                                  ) -> dd.DataFrame:
    """ Read a lazy dask dataframe based on a unique filename string, 
    intended to correspond to a single location (comid) but multiple 
    should work.

    :param dir_db_attrs: Directory where parquet files of attribute data
      stored
    :type dir_db_attrs: str | os.PathLike
    :param fp_struct: f-string formatted unique substring for filename of 
    parquet file corresponding to single location, i.e. f'*_{comid}_*'
    :type fp_struct: str, optional
    :return: lazy dask dataframe of all attributes corresponding to the 
    single comid
    :rtype: dd.DataFrame
    """

    # Based on the structure of comid
    fp = list(Path(dir_db_attrs).rglob('*'+str(fp_struct)+'*') )
    if fp:
      all_attr_ddf = dd.read_parquet(fp, storage_options = None)
    else:
        all_attr_ddf = None
    return all_attr_ddf


def _sub_tform_attr_ddf(all_attr_ddf: dd.DataFrame, 
                        retr_vars: str | Iterable, 
                        func: Callable) -> float:
    """Transform attributes using aggregation function

    :param all_attr_ddf: Lazy attribute data corresponding to a single location (comid)
    :type all_attr_ddf: dd.DataFrame
    :param retr_vars: The basin attributes to retrieve and aggregate by the
      transformation function
    :type retr_vars: str | Iterable
    :param func: The function used to perform the transformation on the `retr_vars`
    :type func: Callable[[Iterable[float]]]
    :return: Aggregated attribute value
    :rtype: float
    """
    sub_attr_ddf= all_attr_ddf[all_attr_ddf['attribute'].isin(retr_vars)]
    attr_val = sub_attr_ddf['value'].map_partitions(func, meta=('value','float64')).compute()
    return attr_val

def _cstm_data_src(tform_type: str,retr_vars: str | Iterable) -> str:
    """Standardize the str representation of the transformation function
    For use in the 'data_source' column in the parquet datasets.

    :param tform_type: The transformation function, provided as a str 
    of a simple function (e.g. 'np.mean', 'max', 'sum') for aggregation
    :type tform_type: str
    :param retr_vars: The basin attributes to retrieve and aggregate by the
      transformation function
    :type retr_vars: str | Iterable
    :return: A str representation of the transformation function, with variables
    sorted by character.
    :rtype: str
    """
    # Sort the retr_vars
    retr_vars_sort = sorted(retr_vars)
    return f"{tform_type}([{','.join(retr_vars_sort)}])"


def _gen_tform_df(all_attr_ddf: dd.DataFrame, new_var_id: str,
                    attr_val:float, tform_type: str,
                    retr_vars: str | Iterable) -> pd.DataFrame:
    """Generate standard dataframe for a custom transformation on attributes
      for a single location (basin)

    :param all_attr_ddf: All attributes corresponding to a single comid
    :type all_attr_ddf: dd.DataFrame
    :param new_var_id: Name of the newly desired custom variable
    :type new_var_id: str
    :param attr_val: _description_
    :type attr_val: float
    :param tform_type: The transformation function, provided as a str 
    of a simple function (e.g. 'np.mean', 'max', 'sum') for aggregation
    :type tform_type: str
    :param retr_vars: The basin attributes to retrieve and aggregate by the
      transformation function
    :type retr_vars: str | Iterable
    :raises ValueError: When the provided dask dataframe contains more than
     one unique location identifier in the 'featureID' column.
    :return: A long-format dataframe of the new transformation variables 
    for a single location
    :rtype: pd.DataFrame
    .. seealso::
        The `proc.attr.hydfab` R package and the `proc_attr_wrap` function
        that generates the standardized attribute parquet file formats
    """
    if all_attr_ddf['featureID'].nunique().compute() != 1:
        raise ValueError("Only expecting one unique location identifier. Reconsider first row logic.")
    
    base_df=all_attr_ddf.loc[0,:].compute() # Just grab the first row of a data.frame corresponding to a  and reset the values that matter
    base_df.loc[:,'attribute'] = new_var_id
    base_df.loc[:,'value'] = attr_val
    base_df.loc[:,'data_source'] = _cstm_data_src(tform_type,retr_vars)
    base_df.loc[:,'dl_timestamp'] = str(datetime.now(timezone.utc))
    return base_df


def proc_tfrm_cfg(tfrm_cfg: list, idx_tfrm_attrs: int,
                   all_attr_ddf: dd.DataFrame) -> pd.DataFrame:
    #TODO Consider removing. Much of this functionality superceded by _retr_cstm_funcs
    # Parse each item in attribute transformation yaml config
    ls_df_rows = []
    for item in tfrm_cfg[idx_tfrm_attrs]['transform_attrs']:
        for key, value in item.items():
            ls_tfrm_keys = list(itertools.chain(*[[*x.keys()] for x in value]))
            idx_tfrm_type = ls_tfrm_keys.index('tform_type')
            idx_var_desc = ls_tfrm_keys.index('var_desc')
            idx_vars = ls_tfrm_keys.index('vars')
            print(f"Transform Name: {key}")
            tfrm_types = value[idx_tfrm_type]['tform_type']
            print(f"Description: {value[idx_var_desc]['var_desc']}")
            retr_vars = value[idx_vars]['vars']

            # TODO Check to see if attribute already exists, if so read here and skip the rest below

            # Perform aggregation

            for tform_type in tfrm_types:
                # Create name of new attribute
                new_var_id = key.format(tform_type=tform_type)
                print(f"Creating {new_var_id}")

                # Convert string to a function
                func = _get_function_from_string(tform_type)

                # Subset data to variables and compute new attribute
                attr_val = _sub_tform_attr_ddf(all_attr_ddf=all_attr_ddf, 
                            retr_vars=retr_vars, func = func)
                
                # Populate new values in the new dataframe
                new_df = _gen_tform_df(all_attr_ddf=all_attr_ddf, 
                                    new_var_id=new_var_id,
                                    attr_val=attr_val,
                                    tform_type = tform_type,
                                    retr_vars = retr_vars)
                
                ls_df_rows.append(new_df)

    df_new_vars = pd.DataFrame(ls_df_rows)
    return df_new_vars

def _retr_cstm_funcs(tfrm_cfg_attrs:dict)->dict:
    # Convert dict from attribute transform config file to dict of the following sub-dicts:

    # dict_all_cstm_vars new custom variable names
    # dict_tfrm_func function design of attribute aggregation & transformation
    # dict_tfrm_func_objs strings denoting function converted to function object
    # dict_retr_vars the standard variables (attrs) needed for each transformation 
    # Each sub-dict's key value corresponds to the new variable name

    dict_retr_vars = dict()
    ls_cstm_func = list()
    ls_all_cstm_vars = list()
    ls_tfrm_funcs = list()
    ls_tfrm_func_objs = list()
    for item in tfrm_cfg_attrs['transform_attrs']:
        for key, value in item.items():
            ls_tfrm_keys = list(itertools.chain(*[[*x.keys()] for x in value]))
            idx_tfrm_type = ls_tfrm_keys.index('tform_type')
            tfrm_types = value[idx_tfrm_type]['tform_type']
            idx_vars = ls_tfrm_keys.index('vars')
            retr_vars = value[idx_vars]['vars']
            for tform_type in tfrm_types:
                ls_tfrm_func_objs.append(_get_function_from_string(tform_type))
                ls_tfrm_funcs.append(tform_type)
                new_var_id = key.format(tform_type=tform_type)
                ls_all_cstm_vars.append(new_var_id)
                ls_cstm_func.append(_cstm_data_src(tform_type,retr_vars))
                dict_retr_vars.update({new_var_id : retr_vars})

    new_keys = list(dict_retr_vars.keys())
    
    dict_all_cstm_vars = dict(zip(new_keys,ls_all_cstm_vars))
    dict_cstm_func = dict(zip(new_keys,ls_cstm_func))
    dict_tfrm_func = dict(zip(new_keys,ls_tfrm_funcs))
    dict_tfrm_func_objs =dict(zip(new_keys,ls_tfrm_func_objs))

    return {'dict_all_cstm_vars': dict_all_cstm_vars,
            'dict_cstm_func':dict_cstm_func,
            'dict_tfrm_func':dict_tfrm_func,
            'dict_tfrm_func_objs':dict_tfrm_func_objs,
            'dict_retr_vars':dict_retr_vars}

def _id_need_tfrm_attrs(all_attr_ddf: dd.DataFrame,
                          ls_all_cstm_vars:list=None,
                          ls_all_cstm_funcs:list=None)->dict:
    # Identify which attributes should be created to achieve transformation goals
    if all_attr_ddf['featureID'].nunique().compute() != 1:
        raise ValueError("Only expecting one unique location identifier. Reconsider first row logic.")

    ls_need_vars = list()
    if ls_all_cstm_vars: 
        existing_attrs_vars = set(all_attr_ddf['attribute'].compute().unique())
        # Generate a list of custom variables not yet created for a single location based on attribute name
        ls_need_attrs = [var for var in ls_all_cstm_vars if var not in existing_attrs_vars]
        ls_need_vars = ls_need_vars + ls_need_attrs
    ls_need_funcs = list()
    if ls_all_cstm_funcs:
        # Generate a list of custom variables not yet created for a single location based on function name
        existing_src = set(all_attr_ddf['data_source'].compute().unique())
        ls_need_funcs = [var for var in ls_all_cstm_funcs if var not in existing_src]
  
    dict_need_vars_funcs = {'vars': ls_need_vars,
                            'funcs': ls_need_funcs}

    return dict_need_vars_funcs

import unittest
from unittest.mock import patch, MagicMock
import pandas as pd
import dask.dataframe as dd
import itertools

class TestTransformationFunctions(unittest.TestCase):

    @patch("your_module._get_function_from_string")
    @patch("your_module._sub_tform_attr_ddf")
    @patch("your_module._gen_tform_df")
    def test_proc_tfrm_cfg(self, mock_gen_tform_df, mock_sub_tform_attr_ddf, mock_get_function_from_string):
        from your_module import proc_tfrm_cfg
        
        # Mock transformation configuration
        tfrm_cfg = [
            {
                'transform_attrs': [
                    {'attr1': [{'tform_type': ['sum']}, {'var_desc': 'Sum of values'}, {'vars': ['var1', 'var2']}]}
                ]
            }
        ]
        
        # Mock index, DataFrame, and function behavior
        idx_tfrm_attrs = 0
        df_mock = pd.DataFrame({"attribute": ["var1", "var2"], "value": [1.0, 2.0]})
        all_attr_ddf = dd.from_pandas(df_mock, npartitions=1)
        mock_sub_tform_attr_ddf.return_value = pd.Series([3.0])
        
        mock_gen_tform_df.return_value = pd.DataFrame({"attribute": ["attr1_sum"], "value": [3.0]})
        
        # Run the function
        result = proc_tfrm_cfg(tfrm_cfg, idx_tfrm_attrs, all_attr_ddf)
        
        # Assertions
        self.assertIsInstance(result, pd.DataFrame)
        self.assertEqual(result["attribute"].iloc[0], "attr1_sum")
        self.assertEqual(result["value"].iloc[0], 3.0)
        
        # Check if internal functions were called
        mock_get_function_from_string.assert_called_once_with("sum")
        mock_sub_tform_attr_ddf.assert_called_once()
        mock_gen_tform_df.assert_called_once()

    def test_retr_cstm_funcs(self):
        from your_module import _retr_cstm_funcs

        # Mock transformation configuration dictionary
        tfrm_cfg_attrs = {
            'transform_attrs': [
                {'attr1': [{'tform_type': ['sum', 'mean']}, {'vars': ['var1', 'var2']}]}
            ]
        }

        result = _retr_cstm_funcs(tfrm_cfg_attrs)
        
        # Assertions
        self.assertIsInstance(result, dict)
        self.assertIn('dict_all_cstm_vars', result)
        self.assertIn('dict_cstm_func', result)
        self.assertIn('dict_tfrm_func', result)
        self.assertIn('dict_tfrm_func_objs', result)
        self.assertIn('dict_retr_vars', result)

        # Verify the specific values in the dictionaries
        self.assertEqual(result['dict_all_cstm_vars'], {'attr1_sum': 'attr1_sum', 'attr1_mean': 'attr1_mean'})
        self.assertEqual(result['dict_cstm_func'], {'attr1_sum': 'sum', 'attr1_mean': 'mean'})
        self.assertEqual(result['dict_tfrm_func'], {'attr1_sum': 'sum', 'attr1_mean': 'mean'})
        self.assertEqual(result['dict_retr_vars'], {'attr1_sum': ['var1', 'var2'], 'attr1_mean': ['var1', 'var2']})

    @patch("dask.dataframe.DataFrame.compute")
    def test_id_need_tfrm_attrs(self, mock_compute):
        from your_module import _id_need_tfrm_attrs

        # Mock Dask DataFrame
        df_mock = pd.DataFrame({
            "featureID": [12345, 12345],
            "attribute": ["existing_attr1", "existing_attr2"],
            "data_source": ["src1", "src2"]
        })
        all_attr_ddf = dd.from_pandas(df_mock, npartitions=1)
        mock_compute.side_effect = [
            pd.Series([1]),  # Simulate single unique location
            pd.Series(["existing_attr1", "existing_attr2"]),
            pd.Series(["src1", "src2"])
        ]
        
        # Define the custom vars and funcs to check for missing
        ls_all_cstm_vars = ["new_attr1", "existing_attr1"]
        ls_all_cstm_funcs = ["src1", "new_src"]

        # Run the function
        result = _id_need_tfrm_attrs(all_attr_ddf, ls_all_cstm_vars, ls_all_cstm_funcs)
        
        # Assertions
        self.assertEqual(result, {
            'vars': ["new_attr1"],  # "existing_attr1" is already present, so only "new_attr1" is missing
            'funcs': ["new_src"]    # "src1" is already present, so only "new_src" is missing
        })

if __name__ == "__main__":
    unittest.main()
